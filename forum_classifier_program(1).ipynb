{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe7f7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mayaechtinaw/Documents/forum-classifier-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mayaechtinaw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mayaechtinaw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mayaechtinaw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/mayaechtinaw/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/mayaechtinaw/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "import contractions\n",
    "import pickle\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.corpus import words\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#os.chdir('Documents/forum-classifier-python')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec4c50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_classifier.pkl', 'rb') as file:\n",
    "    vectorizer = pickle.load(file)\n",
    "    classifier = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474cc8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an input csv file: larger_data.csv\n"
     ]
    }
   ],
   "source": [
    "input_csv = input(\"Enter an input csv file: \")\n",
    "data = pd.read_csv(input_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81812ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                               content\n",
      "0         entry door step seal not installed properly\n",
      "1           reglued the end seals the full wall slide\n",
      "2   got temp plates friday passed emissions friday...\n",
      "3   had the airbags replaced after 20yrs figured w...\n",
      "4   hmm two new chassis batteries new service aqua...\n",
      "..                                                ...\n",
      "70                         working sewer hose storage\n",
      "71                                            content\n",
      "72  took all winter but had replace throttle link ...\n",
      "73  realigned wiper arm parked too low the windshi...\n",
      "74  far this spring removed old tube last year and...\n",
      "\n",
      "[75 rows x 1 columns]>\n"
     ]
    }
   ],
   "source": [
    "contractions = {\n",
    "    \"ain't\": \"am not / are not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall / he will\",\n",
    "    \"he'll've\": \"he shall have / he will have\",\n",
    "    \"he's\": \"he has / he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is\",\n",
    "    \"I'd\": \"I had / I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I shall / I will\",\n",
    "    \"I'll've\": \"I shall have / I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it shall have / it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall / she will\",\n",
    "    \"she'll've\": \"she shall have / she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall / they will\",\n",
    "    \"they'll've\": \"they shall have / they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall / what will\",\n",
    "    \"what'll've\": \"what shall have / what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall / who will\",\n",
    "    \"who'll've\": \"who shall have / who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'alls\": \"you alls\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall / you will\",\n",
    "    \"you'll've\": \"you shall have / you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "contraction_pattern = re.compile(r'\\b(' + '|'.join(contractions.keys()) + r')\\b')\n",
    "\n",
    "def expand_contractions(data, contraction_mapping=contractions):\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        expanded_contraction = contraction_mapping.get(match)\n",
    "        if not expanded_contraction:\n",
    "            expanded_contraction = contraction_mapping.get(match.lower())\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_data = contraction_pattern.sub(expand_match, data)\n",
    "    return expanded_data\n",
    "\n",
    "def remove_word(data, word):\n",
    "    return data.replace(word, '')\n",
    "\n",
    "def remove_URL(data):\n",
    "    url_pattern = re.compile(r'https://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', data)\n",
    "\n",
    "def remove_HTML(data):\n",
    "    html_tags_pattern = r'<.*?>'\n",
    "    return re.sub(html_tags_pattern, '', data)\n",
    "\n",
    "def preprocess_text(data):\n",
    "    stop_words = set(stopwords.words('english'))  # Define stop words\n",
    "    stemmer = PorterStemmer()  # Define stemmer\n",
    "    lemmatizer = WordNetLemmatizer()  # Define lemmatizer\n",
    "    spell = SpellChecker()  # Define spell checker\n",
    "    data['content'] = data['content'].apply(expand_contractions)  # Apply contraction expansion\n",
    "    data['content'] = data['content'].apply(lambda x: remove_URL(x))  # Remove URLs\n",
    "    data['content'] = data['content'].apply(lambda x: remove_HTML(x))  # Remove HTML tags\n",
    "    data['content'] = data['content'].str.lower()  # Lowercase all words\n",
    "    data['content'] = data['content'].apply(lambda x: re.sub(r'[^a-zA-Z0-9 \\s]', ' ', x))  # Remove special characters\n",
    "    data['content'] = data['content'].apply(lambda x: remove_word(x, 'quote'))\n",
    "    data['content'] = data['content'].str.split().map(lambda sl: \" \".join(s for s in sl if len(s) > 2))  # Remove words with length < 2\n",
    "\n",
    "    \n",
    "    #data['tokenized'] = data.apply(lambda row: nltk.word_tokenize(row['content']), axis=1)  # Tokenizing\n",
    "    #data['tokenized'] = data['tokenized'].apply(lambda x: [item for item in x if item not in stop_words])  # Remove stop words\n",
    "    #data['tokenized'] = data['tokenized'].apply(lambda x: [stemmer.stem(y) for y in x])  # Stemming\n",
    "    #data['tokenized'] = data['tokenized'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])  # Lemmatizing\n",
    "    #data['clean_string'] = data['tokenized'].str.join(' ')  # De-tokenizes string\n",
    "    return data\n",
    "\n",
    "data = preprocess_text(data)\n",
    "\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6735de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_tfidf = vectorizer.transform(data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46777a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56701db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "151fc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_mapping = {\n",
    "    0: 'Chassis',\n",
    "    1: 'Door Locks and Parts',\n",
    "    2: 'Electrical',\n",
    "    3: 'Exterior',\n",
    "    4: 'Hardware',\n",
    "    5: 'Heating and Cooling',\n",
    "    6: 'Lights',\n",
    "    7: 'Plumbing and Water',\n",
    "    8: 'Wiper Parts',\n",
    "    9: 'Adhesives, Sealants, and Cleaners',\n",
    "    10: 'Winterizing',\n",
    "    11: 'Interior'\n",
    "}\n",
    "\n",
    "data['prediction'] = data['prediction'].apply(lambda x: product_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e0dea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('forum_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1e4b672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                               content            prediction\n",
      "0         entry door step seal not installed properly  Door Locks and Parts\n",
      "1           reglued the end seals the full wall slide              Exterior\n",
      "2   got temp plates friday passed emissions friday...                Lights\n",
      "3   had the airbags replaced after 20yrs figured w...              Exterior\n",
      "4   hmm two new chassis batteries new service aqua...    Plumbing and Water\n",
      "..                                                ...                   ...\n",
      "70                         working sewer hose storage    Plumbing and Water\n",
      "71                                            content              Exterior\n",
      "72  took all winter but had replace throttle link ...              Exterior\n",
      "73  realigned wiper arm parked too low the windshi...              Exterior\n",
      "74  far this spring removed old tube last year and...    Plumbing and Water\n",
      "\n",
      "[75 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf7247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
